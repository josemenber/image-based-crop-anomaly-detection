from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.inception_v3 import InceptionV3
from keras.applications.xception import Xception
from keras.applications.resnet50 import ResNet50
from keras.applications.vgg16 import VGG16
from keras.layers import Dense, GlobalAveragePooling2D
from sklearn.model_selection import train_test_split
from keras.models import Model, load_model
from skimage.transform import resize
from urllib.request import urlopen
from keras.optimizers import Adam
from keras import backend as K
from skimage.io import imread
from zipfile import ZipFile
from datetime import datetime
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sys
import csv
import argparse
import os
import requests
import cv2
import pytz



# Script arguments
def parse_args():
    parser = argparse.ArgumentParser(description='Train and test VGG16, ResNet50, InceptionV3 and Xception models on DeepWeeds or Agriculture-Vision datasets.')
    parser.add_argument("--agriculture", action='store_true', default=False, help="Train models on the Agriculture-Vision 2020 dataset. By default it uses the DeepWeeds dataset.")
    parser.add_argument("--noise", action='store_true', default=False, help="Apply noise to images.")
    parser.add_argument("--augmentation", action='store_true', default=False, help="Apply data augmentation to images.")
    parser.add_argument('--model', default='xception', help="'vgg16', 'resnet-50', 'inception-v3' or xception.")
    parser.add_argument('--max_epochs', type=int, default=500, help="Maximum number of epochs.")
    args = parser.parse_args()
    return args.agriculture ,args.noise, args.augmentation, args.model, args.max_epochs

# Read arguments
(agriculture, gaussian_noise, data_augmentation, modelname, MAX_EPOCHS) = parse_args()


SEED_VALUE = 1  # Set the seed for reproducibility

# Add gaussian noise function
# param img: Input image
# return: img with gaussian noise
def add_noise(image):
    row,col,ch = image.shape
    mean = 0
    var = 0.1
    sigma = var**0.5
    gauss = np.random.normal(mean,sigma, (row,col,ch))
    gauss = gauss.reshape(row, col, ch)
    noisy = image + gauss
    return noisy

def crop(img, size):
    """
    Crop the image concentrically to the desired size.
    :param img: Input image
    :param size: Required crop image size
    :return:
    """
    (h, w, c) = img.shape
    x = int((w - size[0]) / 2)
    y = int((h - size[1]) / 2)
    return img[y:(y + size[1]), x:(x + size[0]), :]


def crop_generator(batches, size):
    """
    Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator
    :param batches: Batches of images to be cropped
    :param size: Size to be cropped to
    :return:
    """
    while True:
        batch_x, batch_y = next(batches)
        (b, h, w, c) = batch_x.shape
        batch_crops = np.zeros((b, size[0], size[1], c))
        for i in range(b):
            batch_crops[i] = crop(batch_x[i], (size[0], size[1]))
        yield (batch_crops, batch_y)


# Agriculture dataset
if(agriculture):
    df = pd.read_csv("train.csv")
    
    num_classes = [0, 1, 2, 3, 4, 5, 6]
    classes = ['cloud_shadow', 'double_plant', 'planter_skip', 'standing_water', 'waterway', 'weed_cluster', 'background']
    
    # Split data: train (60%), validation(15%) and test (25%)
    train_index, val_index = train_test_split(df.index, test_size=0.2, random_state=SEED_VALUE, shuffle=True, stratify=df['Label'])
    train_dataframe = df.loc[train_index, ]
    val_dataframe = df.loc[val_index, ]
    test_dataframe = pd.read_csv("val.csv")
    test_index = test_dataframe.index

    train_directory = './train_images/'
    val_directory = './train_images/'
    test_directory = './val_images/'
    img_size = (512, 512)
    crop_size = (512, 512)
    model_input = (512, 512, 3)
    zoom = 0.0 # No zoom

# DeepWeeds dataset
else:
    df = pd.read_csv("./labels/labels.csv")

    num_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8]
    classes = ['Chinee apple', 'Lantana', 'Parkinsonia', 'Parthenium', 'Prickly acacia', 'Rubber vine', 'Siam weed', 'Snake weed', 'Negative']

    # Split data: train (60%), validation(20%) and test (20%)
    train_index, test_index = train_test_split(df.index, test_size=0.4, random_state=SEED_VALUE, shuffle=True, stratify=df['Label'])
    val_index, test_index = train_test_split(test_index, test_size=0.5, random_state=SEED_VALUE, shuffle=True, stratify=df.loc[test_index, 'Label'])

    train_dataframe = df.loc[train_index, ]
    val_dataframe = df.loc[val_index, ]
    test_dataframe = df.loc[test_index, ]
    
    train_directory = './images/'
    val_directory = './images/'
    test_directory = './images/'
    img_size = (256, 256)
    crop_size = (224, 224)
    model_input = (224, 224, 3)
    zoom = (0.5, 1)
    

if(data_augmentation):
    if(os.path.exists('new_dataframe.csv')):
        new_df = pd.read_csv('new_dataframe.csv')
        train_dataframe = pd.concat([train_dataframe, new_df])
    else:
        sys.exit('System error: To apply training set augmentation you have to generate them first. Use the Python script generate_images.py')

# Train image augmentation with and without gaussian noise
if(gaussian_noise):
    train_data_generator = ImageDataGenerator(rescale=1./255,
                                              fill_mode="constant",
                                              shear_range=0.2,
                                              zoom_range=zoom,
                                              horizontal_flip=True,
                                              rotation_range=360,
                                              channel_shift_range=25,
                                              brightness_range=(0.75, 1.25),
                                              preprocessing_function=add_noise)
else:
    train_data_generator = ImageDataGenerator(rescale=1./255,
                                              fill_mode="constant",
                                              shear_range=0.2,
                                              zoom_range=zoom,
                                              horizontal_flip=True,
                                              rotation_range=360,
                                              channel_shift_range=25,
                                              brightness_range=(0.75, 1.25))
    
# No testing image augmentation (except for converting pixel values to floats)
test_data_generator = ImageDataGenerator(rescale=1./255)

# Load train images in batches from directory and apply augmentations
train_data_generator = train_data_generator.flow_from_dataframe(train_dataframe,
                                                                directory=train_directory,
                                                                x_col='Filename',
                                                                y_col='Species',
                                                                target_size=img_size,
                                                                batch_size=32,
                                                                seed=SEED_VALUE,
                                                                shuffle=True,
                                                                class_mode='categorical')

# Load validation images in batches from directory and apply rescaling
val_data_generator = test_data_generator.flow_from_dataframe(val_dataframe,
                                                            directory=val_directory,
                                                            x_col="Filename",
                                                            y_col="Species",
                                                            target_size=crop_size,
                                                            batch_size=32,
                                                            shuffle=False,
                                                            seed=SEED_VALUE,
                                                            class_mode='categorical')

# Load test images in batches from directory and apply rescaling
test_data_generator = test_data_generator.flow_from_dataframe(test_dataframe,
                                                              directory=test_directory,
                                                              x_col="Filename",
                                                              y_col="Species",
                                                              target_size=crop_size,
                                                              batch_size=32,
                                                              shuffle=False,
                                                              seed=SEED_VALUE,
                                                              class_mode='categorical')

if(not agriculture):
    # Crop augmented images from 256x256 to 224x224 for DeepWeeds
    train_data_generator = crop_generator(train_data_generator, crop_size)

# Load ImageNet pre-trained model with no top, either VGG16, InceptionV3, ResNet50 or Xception (default)
if(modelname == 'inception-v3'):
    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=model_input)
elif(modelname == 'resnet-50'):
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=model_input)
elif(modelname == 'vgg16'):
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=model_input)    
else:
    base_model = Xception(weights='imagenet', include_top=False, input_shape=model_input)
    
x = base_model.output

# Add a global average pooling layer
x = GlobalAveragePooling2D(name='avg_pool')(x)
# Add fully connected output layer with sigmoid activation
outputs = Dense(len(num_classes), activation='sigmoid', name='fc')(x)
# Assemble the modified model
model = Model(inputs=base_model.input, outputs=outputs)

# Checkpoints for training
checkpoint = ModelCheckpoint("./best_model.hdf5", monitor='val_loss', mode='min', verbose=1, save_best_only=True)
early_stopping = EarlyStopping(patience=250, monitor='val_loss', mode='min', restore_best_weights=True)
tensorboard = TensorBoard(log_dir="./results/", histogram_freq=0, write_graph=True, write_images=True)
reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=20, min_lr=6.25e-6)
csv_logger = CSVLogger("./training_metrics.csv")

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])

 # Train model until MAX_EPOCHS or early stopping
np.random.seed(SEED_VALUE)
H = model.fit(train_data_generator,
              steps_per_epoch=len(train_dataframe.index)//32,
              epochs=MAX_EPOCHS,
              validation_data=val_data_generator,
              validation_steps=len(val_index)//32,
              callbacks=[checkpoint, early_stopping, tensorboard, reduce_lr, csv_logger],
              shuffle=False,
              verbose=1)

# Load the best model
model = load_model("./best_model.hdf5")

# Evaluate model on test subset
predictions = model.predict_generator(test_data_generator, len(test_index)//32+1)
y_true = test_data_generator.classes
y_pred = np.argmax(predictions, axis=1)

# Generate and print classification metrics and confusion matrix
print()
print(accuracy_score(y_true, y_pred))
print()
print(confusion_matrix(y_true, y_pred))
print()
print(classification_report(y_true, y_pred))

